---
title: "[Paper Review] GAN"
date: 2024-01-09 16:30:11 +/-0000
categories: [Paper Review, GenerativeAI]
tags: [gan, ai, generative, math]   
math: true  # TAG names should always be lowercase
typora-root-url: ../
---

# GAN(Generative Adversarial Nets, 2015)

### Introduction

기존의 생성모델들에서는 Maximum likelihood estimation(최대 우도 추정)에서 발생하는 여러 불가능한 확률 계산을 근사화해야하는 어려움이 있었다. 이러한 문제를 우회하며 좋은 품질의 컨텐츠를 생성하기 위해 적대적 생성 네트워크인 GAN이라는 접근 방식을 제안한다.



### Adversarial Nets

Adversarial 모델링을 진행하기 위해 두개의 다층 퍼셉트론 모델을 정의한다. 하나는 Generator(생성자 ; $G$), 다른 하나는 Discriminator(판별자 ; $D$)이다.

데이터 $x$에 대한 $G$의 분포 $p_g$를 학습하기 위해서 노이즈로 이루어진 변수 $p_z (z)$를 정의한다. 그런 다음, 데이터 공간과의 mapping을 $G(z;\theta_g)$로 표현한다. 

또다른 다층 퍼셉트론 모델 $D$는 $D(x;\theta_d)$로 표현되며 $D(x)$는 $G$의 분포인 $p_g$가 아닌 데이터에서 $x$가 나올 확률을 출력한다. 즉, $G$가 만든 것인지, 실제 데이터에서 나온 것인지 구분하는 역할이다. $G$가 생성한 샘플이 실제와 유사할수록, $D$의 값은 커진다.

GAN은 두 모델을 다음과 같이 **동시에** 훈련한다.

* $D$는 실제 데이터와 $G$가 생성한 sample을 정확히 분류하는 방향으로 훈련.
* $G$는 $log(1-D(G(z)))$를 최소화하는 방향으로 훈련. 즉, 생성한 샘플 $G(z)$를 $D$가 분류하였을 때, $D$의 값이 커지는 방향으로 훈련한다.

이는 다음과 같은 Value function을 따르는 $minmax$ 게임을 진행한다.
$$
\underset{G}{min}\underset{D}{max}\;V(D, G) = \mathbb{E}_{x \sim p_{data}
(x)}[log\;D(x)] + \mathbb{E}_{z\sim p_z(z)}[log(1-D(G(z)))]
$$

>  $\mathbb{E}_{x \sim p_data(x)}$의 의미는 Noise 분포인 $p_{data}(x)$를 따르는 $x$의 기댓값을 의미한다.

Adversarial Network를 학습하는 과정에서 $D$를 완벽하게 optimizing하는 것을 방지한다. 같은 step이 진행되었을 때 $D$의 성능이 $G$에 비하여 월등히 좋아지기 때문에 $G$의 학습이 원활하게 이루어지지 않기 때문이다. 이를 방지하기 위해, $D$의 Optimizing하는 $k$ 단계와 $G$를 Optimizing하는 하나의 단계를 묶어서 진행한다.

이는 SML/PCD 전략과 유사한데, Markov chain에서 Burn in 문제를 피하기 위해 Markov chain의 sample을 다음 스텝까지 유지하는 전략이다. 

> Markov chain이란 Markov 성질을 가진 이산 시간 확률 과정이며, Markov 성질은 미래 상태의 조건부 확률 분포는 과거 상태와는 무관하게 현재 상태에 의해서만 결정된다는 것이다. 즉 어떠한 time step $t$가 있다면 다음 time step인 $t+1$에는 1 ~ $t-1$에는 상관없이 현재 $t$에 대해서만 결정된다는 것이다.

> Burn in 이란 Markov chain 특정 시점까지의 초기 상태 또는 관찰들을 무시하는 과정을 말한다. 이 과정은 마르코프 체인이 정상 분포에 수렴하는 데 필요한 시간을 나타내며 이 과정동안 생성된 샘플들은 폐기된다.

> SML(Stochastic Maximum Likelihood)는 확률적 최대우도 추정법이라고 불린다. SML은 기본적으로 데이터의 우도를 최대화하는 모델의 파라미터 값을 찾는 기법이다.
>
> PCD(Persistent Contrastive Divergence)는 특정한 유형의 확률적 네트워크 모델을 학습하는 데 사용되는 알고리즘이다. ~~SML과 PCD는 따로 공부하여 post를 추후 올릴 예정이다..~~

실제로 식 (1)은 초기 학습단계에서 $G$가 학습하기 충분한 gradient를 제공하지 못해 학습이 잘 이루어지지 않는다. $G$의 성능이 낮아 생성된 샘플을 $D$는 확신을 가지고 분류할 수 있으며, 이 과정이 반복되면 $G$가 minimize해야하는 $log(1-D(G(z)))$ 식은 포화상태에 이르러 gradient가 소실되는 문제가 발생한다. 이에 대한 대안으로 $log(1-D(G(z)))$를 **minimize**하는 대신,  $log\;D(G(z))$를 **maximize** 하는 방향으로 목적함수를 개선하여 학습 초기에 더 강한 gradient를 제공한다.

> Gradient의 변화가 있는 이유는 $log(1-D(G(z)))$를 minimize 할때 $G$의 성능이 안좋아 $D(G(z))$의 값이 0에 가까워지고, 결국 $log(1)$, 즉 0에 가까워져 Gradient 값이 완만하여 학습 속도가 많이 낮아지기 때문이다. $log\;D(G(z))$를 maximize할 시 gradient값이 가파르게 되어 학습이 빠르게 진행될 수 있다.

![figure1](/assets/img/GAN/figure1.png)

Figure 1: GAN은 <span style='color:green'>**generative 분포**</span>와 **data generating 분포**를 구별하기 위해 <span style='color:blue'>**discriminative 분포**</span>를 동시에 학습하며 업데이트한다.
