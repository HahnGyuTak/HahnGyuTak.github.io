---
title: "(Writing)[Paper Review] TADA : Timestep-Awara Data Augmentation for Diffusion models"
date: 2024-04-02 12:40:11 +/-0000
categories: [Paper Review, Diffusion]
tags: [ai, generative, math, diffusion]   
math: true  # TAG names should always be lowercase
typora-root-url: ../../../
---



[논문 링크](https://openreview.net/forum?id=U6Mb3CRuj8)

데이터 증강(Data Augmentation)은 주어진 원본 데이터를 확장하여 데이터셋의 다양성을 증가시키는 기법이다. 이 방법은 특히 학습 데이터가 부족한 경우, 모델의 일반화 능력을 향상시키기 위해 사용한다.

이번 논문에서는 Diffusion model의 distribution 변화가 특정 timestep에서 발생한다는 것을 발견하고 timestep에 따라 유연하게 강도를 조정하는 Data augmentation 전략을 제안한다.

이러한 전략이 다양한 diffusion model에 도움되기를 바란다고 저자들은 말한다.

## **Introduction**

Diffusion 모델의 reverse process에서 데이터 증강이 어떤 영향을 미치는지 살펴보았을 때, 성능 저하에 영향을 미치는 timestep을 파악하고, 특정 timestep이 sampling 과정의 변형에 기여하여 의도하지 않은 sample 생성이 이루어진다는 것을 파악하였다.

위에서 파악한 것을 바탕으로 저자들은 timestep에 따라 증강 강도를 유연하게 조정하는 Timestep-Aware Data Augmentation (TADA) 전략을 제안한다.

위 전략은 $T(x_t, w_t)$로 표시되는데, timestep $t$(noise level이 되기도 함)에 따라 증강 강도 $w_t$를 조절한다. 즉, input 데이터에 큰 noise가 포함된 경우 증강 강도가 강한 train sample($w_t$)로 model을 train한다. vulnerable한 timestep 동안에는 0에 가까운 $w_t$를 적용하다가 다시 약한 noise가 포함된 data가 input이 들어오면 다시 강도를 높인다.



## **Method**

### **Preliminaries**

TADA를 살펴보기 전 필요한 여러 개념들을 살펴보자

#### **Diffusion models**

$n$개의 데이터 point ${x_0^1, \cdots, x_0^n}$가 분포 $q(x_0)$에서 sampling되었다고 가정하자. Diffusion model은 분포 $q(x_0)$에 가장 근접한 모델 $p_θ(x_0)$를 만드는 것이 목표이다.

Diffusion model을 training하는 것에는 2가지 process가 있다.

* foward process : Gaussian noise $z \sim \mathcal{N}(0,\textit{I})$를 timestep $t$에 따라 추가하여 noise data $x_t$를 만드는 과정
* reverse process : foward process에서 추가된 noise를 제거(denoising)하여 원래의 데이터를 복구하는 과정



Foward process에서는 $x_0$를 이용해 $x_t = \alpha_t x_0 + \sigma^2 z$를 계산한다. 모델 $\widehat{\epsilon}_\theta (x_t, t)$는 weighted MSE를 최소화하여 timestep $t$에서 추가된 noise $z$를 예측하도록 train된다.

Reverse process에서는 $p\_\theta(x_{t-1}\|x_t)$를 통해 $x_t$로 $x_{t-1}$을 계산한다. 이 과정은 $x_T \sim \mathcal{N}(0, \textit{I})$, 즉 완전한 Gaussian noise인 상태에서 시작한다. $\widehat{x}\_\theta (x_t, t) = \frac{x_t - \sigma_t^2 \widehat{\epsilon}_\theta (x_t, t)}{\alpha_t}$일 때, reverse process $x_{t-1}$는 다음과 같다.


$$
\widehat{x}_{t-1} = \alpha_{t-1} \widehat{x}_\theta(x_t, t) + \sigma_{t-1}^2z \tag{1}
$$


$\alpha_t$와 $\sigma_t^2$, Objective, sampline method 등에는 다양한 방법이 있지만, 이 논문에서는 DDPM의 향상된 버전인 Improved DDPM에서 제안한 방법을 채택하였다.



#### **Signal-to-nosie ratio(SNR)**

[Variational Diffusion Models](https://arxiv.org/abs/2107.00630)에서는 signal-to-noise ratio(SNR)이라는 개념을 도입했으며,  이는 각 timestep $t$에서 noise level을 측정하는 개념이다. SNR은 $t$에 따라 점점 감소하는 함수이다. ($\text{SNR}(t) = \frac{\alpha_t^2}{\sigma_t^2}$)



#### **Data augmentation**

이 논문에서는 데이터 증강을 $T(x_t, w)$로 표시하며, $w \in [0,1]$은 증강의 강도를 제어하는 정규화된 hyper parameter이다.



### **Analyzing the effect of data augmentation on learned reverse process**

데이터 증강이 Diffusion model에 미치는 영향을 파악하고 취약한 $t$를 파악하기 위해 증강을 통해 학습된 model과 일반적인 model의 reverse process를 비교한다.

![Figure 1](/assets/img/TADA for Diffusion/Figure 1.png)

위 이미지에서 **(a)**는 horizontal-flip 으로만 train된 baseline 모델 ($\widehat{ε}\_{\text{base}}$)과 증강 데이터로 train된 모델인 ($\widehat{ε}_{\text{aug}}$)의 두 Diffusion model의 reverse process를 비교한 실험의 결과이다. **(a)**그래프를 보면 rough(초기)와 fine(마지막) 시점에서는 두 모델의 차이(<span style='color: #F7DDBE'>노란선</span>)가 거의 없으며, sensitive한 timestep에서는 차이가 벌어짐을 알 수 있다.



**(b)**는 **(a)**의 sensitive timestep에서 $\widehat{ε}\_{\text{aug}}$와 $\widehat{ε}\_{\text{base}}$ 사이의 reverse process를 $\widehat{ε}\_{\text{base}} \rightarrow \widehat{ε}\_{\text{aug}}$ 와 $\widehat{ε}\_{\text{aug}} \rightarrow \widehat{ε}\_{\text{base}}$로 바꿔서 두 가지 sampling process를 생성한 결과이다.

* 처음 두 행은 sample이 처음에는 데이터 분포를 따르지만 결국 증강된 데이터 분포에 포함되는 $\widehat{ε}\_{\text{base}} \rightarrow \widehat{ε}\_{\text{aug}}$ 의 경우를 보여준다. 즉, sample이 $\widehat{ε}\_{\text{base}}$에 의한 데이터 분포를 따르더라도 $\widehat{ε}_{\text{aug}}$에 의해 변경되어 증강된 것과 같은 결과물로 다시 나타난다. 
* 마지막 두 행은 처음에 $\widehat{ε}\_{\text{aug}}$의 궤적을 따르던 샘플이 sensitive timestep 동안 $\widehat{ε}\_{\text{base}}$에 의해 조정되어 궁극적으로 데이터 분포와 일치하게 된 경우를 보여준다.

> Diffusion model의 sensitive timestep에서 sampling trajectory을 변경하여 최종 sample이 원래 데이터 분포를 따르는지 아니면 증강된 분포를 따르는지를 결정할 수 있음을 알 수 있다.



### **Timestep-Aware Data Augmentation for Diffusion models**

위 실험을 바탕으로 논문에서는 rough와 fine timestep에서는 강력한 증강(큰 $w$)을 적용하고, sensitive timestep에서는 강도를 낮추는 전략을 제안한다.

* sensitive timestep($t \in [t_{rough}, t_{fine}]$): 약한 강도의 증강이 train 데이터에 적용되어 생성된 sample $p(x)$에 가깝게 유지
- rough($t \in (t\_{rough},T]$) 및 fine($t \in [0,t_{fine})$) timestep: 강력한 증강을 통해 과적합을 방지하고 확산 모델의 일반화 기능을 개선

증강 강도 $w_t$는 다음과 같이 구할 수 있다.


$$
w_t = k(r_t - r_{rough})(r_t - r_{fine}) + \delta \tag{2}
$$




윤상두

- 
