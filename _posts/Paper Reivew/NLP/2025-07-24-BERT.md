---
title: "[Paper Review] BERT"
date: 2025-07-23 14:35:43 +/-0000
categories: [Paper Review, NLP]
tags: [ICLR, NLP, LLM, Google, BERT, NAACL, Transformer]   
use_math: true  # TAG names should always be lowercase
typora-root-url: ../../../
---

# **[논문 리뷰] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**

> **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**
>
> NAACL 2019
>
> [[Arxiv](https://arxiv.org/abs/1810.04805)] [[Github](https://github.com/google-research/bert)]

**BERT**(**B**idirectional **E**ncoder **R**epresentations from **T**ransformers)는 구글 AI Language 팀에서 발표한 논문으로, 양방향(bidirectional) Transformer를 pretrain함으로써 NLP 분야의 다양한 과제에서 성능을 획기적으로 향상시켰다.

이후 NLP 연구의 표준 모델로 자리잡았다.



## **Introduction**

NLP 분야에서는 **pretrained 모델**을 활용한 Transfer Learning이 다양한 downstream task에서 성능을 향상시키는 것으로 나타났다 . 

기존에는 이러한 접근에 두 가지 방식이 있었다. 

1. feature-based

   * pretrained 모델이 출력한 **맥락 임베딩(contextual embedding)**을 별도의 모델 input feature로 사용

   * 예시 : **ELMo 모델**

     * 전방향과 역방향 LSTM 언어 모델을 각각 훈련하여 얻은 두 방향의 표현을 shallow concatenation(얕게 연결)하여 단어의 문맥 표현으로 사용하는 방식

       > task별 모델에 pretrain representation을 추가 특성으로 넣어줌

     * 질문응답, 감성분석, 개체명 인식 등 여러 task에서 성능 향상

     * **Limitation**

       * 모든 layer에서 양방향 정보를 깊게 융합하지 못함

         > 모델 내부의 중간 표현들은 여전히 단일 방향에 치우쳐 학습

       * 여전히 task별 아키텍처 설계와 추가 파라미터 튜닝이 필요

         > 복잡한 과제일수록 별도 모델 설계와 하이퍼파라미터 조정 부담 증가

2. fine-tuning

   * pretrained 모델에 아주 적은 task별 parameter만 추가하고 **전체 모델을 downstream task 데이터로 fine-tuning**하는 방식

   * 예시 : **OpenAI의 GPT**

     * Transformer 기반 언어 모델을 한 방향(좌->우)으로 학습시킨 후 다양한 문장 단위 과제에 end-to-end fine-tuning하여 좋은 성과를 거둠

     * **Limitation**

       * 각 token이 오른쪽 문맥(right context) 을 전혀 보지 못하므로, 문장 전체 맥락을 활용한 표현 학습에 제약

         > token 수준 task(개체명 인식, 질의응답 등)에서 성능 하락

       * 좌우 문맥 정보를 동시에 활용해야 하는 문장 관계 추론 또는 문장 내부의 정보 상호작용을 깊이 있게 학습이 어려움

     

**위와 같이 기존 언어 모델들은 공통적으로 단방향 제약이 있다.** 



BERT는 **ELMo**와 **GPT**의 장점을 취하되 한계를 극복한 모델이다. 

ELMo처럼 **양방향 정보**를 쓰지만 더 **깊이 통합된 표현**을 사용하고, GPT처럼 **파인튜닝**으로 간단히 downstream task에 적용되지만 **양방향 Transformer 인코더** 구조를 갖는다 . 

![스크린샷 2025-07-22 오후 5.29.19](/assets/img/2025-07-24-BERT/fig3.png)

위 이미지는 BERT, GPT, ELMo를 비교한 그림이다.

### **Contribution**

BERT의 Contribution은 다음과 같이 정리할 수 있다.

* Pretrain 목표 1 : Masked Language Model(**MLM**)을 통한 깊은 bidirectional 문맥 학습

  * 단순히 양쪽 문맥을 동시에 참조하면 모델이 각 단어를 예측할 때 자기 자신을 보는 문제가 발생

  * MLM 사전 학습: 입력 문장의 일부(전체 토큰의 15%)를 무작위로 $[\text{MASK}]$ 토큰으로 가린 후, 모델이 주변 문맥만으로 해당 단어를 맞추도록 학습

    > **Cloze Task**
    >
    > 1953년 Taylor가 제안한 평가 방식으로, 문장에서 일정 비율의 단어를 가리고 빈칸에 알맞은 단어를 채워 넣도록 하여 독자의 어휘력ㆍ문법 지식ㆍ맥락 이해력을 측정
    >
    > BERT의 MLM은 이 Cloze 방식을 차용하여, 딥러닝 모델이 양방향 문맥을 깊게 학습하도록 함

  * 모든 layer에서 단어의 좌우 문맥 정보를 융합한 표현 학습을 가능하게 함

* Pretrain 목표 2 : Next Sentence Prediction(**NSP**)

  * 문장 쌍 간의 관계를 학습 (문장 A와 B가 실제로 연속되는지 이진 분류)

    > 문장 쌍 관계 정보를 pretrain 단계에서 미리 학습

  * 자연어 추론(NLI)이나 질의응답처럼 문장 간 관계 이해가 중요한 task에서 성능 상승

  * NSP를 통해 $[\text{CLS}]$ 토큰이 문장 관계 표현을 내포하도록 유도

  * 이전 모델들이 다루지 못한 문장 간 논리 관계 등을 학습

* **통합 Architecture & 간단한 Fine-tuning**

  * 모든 task에 대해 단일 Transformer Encoder 사용

  * task별 파라미터는 한두개의 출력 layer만 추가해 fine-tuning

    > ELMo처럼 복잡한 과정 없이, GPT처럼 간단한 fine-tuning만으로 **문장 수준·토큰 수준** task 수행 가능

  * *$[\text{CLS}]$, $[\text{SEP}]$* 토큰과 segment 임베딩을 활용해 **하나의 통합된 입력 표현**



BERT는 11개의 NLP task들에서 당시 SOTA를 달성하였다.





## **BERT**

![스크린샷 2025-07-22 오후 5.29.19](/assets/img/2025-07-24-BERT/fig1.png)

 프레임워크는 Pre-training과 Fine-tuing의 2단계로 이루어져있다.

Pre-training을 통해 모델은 다양한 사전 학습 작업을 통해 unlabeled data로 학습된다. 

Fine-tuning의 경우, 먼저 pretrained 파라미터로 BERT 모델을 초기화하고, downstream task의 labeled data를 사용하여 학습한다.



**Model Architecture**

BERT는 여러 개의 **Self-Attention 기반 Transformer Block**을 쌓은 Language Model이다. 

논문에서는 두 가지 모델 크기를 다룬다.

* **$\textbf{BERT}_\textbf{BASE}$**(약 1억 1천만 파라미터)
  * 12개 layer, Hidden state 768, Attention Head 12개
    * L=12, H=768, A=12
  * OpenAI GPT와 직접 비교하기 위해 유사한 규모로 세팅
* **$\textbf{BERT}_\textbf{LARGE}$**(약 3억 4천만 파라미터) 
  * 24개 layer, Hidden state 1024, Attention Head 16개
    * L=24, H=1024, A=16

BERT는 bidirectional self-attention을 사용하여 unidirectional 인 GPT보다 **맥락 활용 능력**에서 BERT는 GPT보다 훨씬 유연하고 강력한 표현을 학습할 수 있게 된다.



**Input Representation**

BERT 모델은 하나의 **통합 아키텍처**로 다양한 다운스트림 작업을 다룰 수 있도록 설계되었다. 

이를 위해 Input Representation은 **token 임베딩 + segment 임베딩 + position 임베딩**으로 구성된다.

Input으로 한 개 또는 두 개의 문장을 연달아 넣을 수 있다. 이때,

* 모든 입력 시퀀스의 첫 번째 token으로는 항상 $[\text{CLS}]$라는 분류 token을 추가
  * $[\text{CLS}]$ token에 대응되는 최종 은닉 벡터 $C$는 시퀀스 전체의 표현으로 간주
* 문장 두 개가 Input인 경우, 첫 번째 문장 끝에 $[\text{SEP}]$ token을 추가해서 문장을 구분
  * 각 token에는 segment 임베딩 $E_A$ 혹은 $E_B$를 더해주어, 해당 token이 첫 번째 문장(A)인지 두 번째 문장(B)인지 식별

![스크린샷 2025-07-22 오후 5.52.26](/assets/img/2025-07-24-BERT/fig2.png)

최종적으로 위와 같이 Transformer Encoder에 입력되는 벡터 표현이 만들어진다 .

> 위 그림은 BERT 입력 표현의 예시이다. 
>
> 하나의 입력 시퀀스에 두 문장 **“My dog is cute” (문장 A)**와 **“He likes playing” (문장 B)**가 $[\text{SEP}]$으로 구분되어 연결되어 있다. $[\text{CLS}]$ token이 맨 앞에 추가되고, 각 token에는 해당 문장의 segment 정보(A or B)와 절대 위치정보가 더해져 모델에 입력된다. 



이러한 Input representation 덕분에, BERT는 **질문-답변 쌍, 문장 쌍 분류, 문맥 내 단일 문장 태깅** 등 다양한 입력 구성을 일관되게 처리할 수 있다.





### **Pre-Training**

BERT의 pretraining 단계에서는 **두 가지** un-supervised learning task를 동시에 수행한다.



#### **Masked Language Model (MLM)**

입력 문장에서 일부 token을 가리고 해당 위치의 **원래 단어를 예측**하는 과정이다.

> 예를 들어,
>
>  “The $[\textbf{MASK}]$ sat on the $[\textbf{MASK}]$.”
>
> 과 같은 입력이 주어지면, 모델은 첫 번째가 “cat”, 두 번째가 “mat”였음을 맞추는 것

이를 통해 BERT는 **좌우 문맥을 모두 고려**하여 단어의 의미를 추론하는 능력을 갖추게 된다. 



논문에서는 **전체 token의 15%**를 무작위로 선택해 $[\textbf{MASK}]$ 처리를 했으며, 이렇게 마스크된 token 위치의 최종 표현 벡터 $T_i$를 **softmax 분류기 layer**를 통해 원래 단어의 어휘 ID로 예측하도록 학습시켰다. 

$[\textbf{MASK}]$ token은 실제 downstream task fine-tuning에서는 등장하지 않는 token이기 때문에, 오직 $[\textbf{MASK}]$만 사용하면 pretrain과 fine-tuning 간 차이가 발생하는 문제점이 있다 . 

이를 완화하고자 BERT는 마스크 선택된 자리의 단어를 다음과 같은 비율로 치환한다.

* 80% : $[\textbf{MASK}]$
* 10% : 무작위 다른 단어
* 10% : 원래 단어 그대로

이렇게 하면 모델이 때로는 실제 단어를 보기도 하기 때문에 $[\textbf{MASK}]$ token에 과도하게 의존하는 것을 막고, Fine-tuning 할 때에도 학습된 표현이 잘 작동하도록 한다.



> $[\textbf{MASK}]$ 위치의 **은닉 벡터** $h_\text{masked}$ 가 나오는데, 이 벡터를 사용하여 
>
> 1. $h_\text{masked}$ (크기 $H$) × 어휘(vocabulary) 크기 $V$의 **가중치 행렬** $W$ (shape $V×H$) → $V$차원 logit 벡터 생성
> 2. logit에 softmax 적용 → 각 어휘에 속할 **확률 분포** $p(\text{token} \| \text{문맥})$
> 3. 정답(실제 가려진 단어)의 인덱스만 골라 cross-entropy loss
>
> → 이 과정을 가려진 모든 위치(15% token)에 대해 수행



#### **Next Sentence Prediction (NSP)**

두 문장이 주어졌을 때 **두 번째 문장이 첫 번째 문장의 다음에 실제로 이어지는 문장인지**를 판단하는 **이진 분류** 과제이다. 

자연어 추론(NLI)이나 질의응답(QA) 같은 task는 두 문장의 관계 이해가 핵심인데, 순수 언어 모델링만으로는 이러한 관계 정보를 직접 학습하기 어렵다. 그래서 BERT는 대용량 텍스트로부터 손쉽게 생성할 수 있는 문장 쌍 데이터를 이용하여, 두 문장이 이어지는지 여부를 맞추는 훈련을 수행한다 . 

문장 A와 B를 뽑아 한 쌍의 입력으로 구성할 때, 다음과 같이 구성한다.

* 50%는 B를 A 다음에 등장하는 연속 문장($\text{IsNext}$)
* 50%는 B를 다른 문장으로 바꿔 무작위 문장($\text{NotNext}$)

모델은 $[\textbf{CLS}]$ token의 최종 벡터 $C$를 입력으로 하는 binary classifier layer를 통해 두 문장의 연속 여부를 예측하도록 학습된다.

이로써 모델이 문장 사이의 의미 연결이나 순서 관계를 내재적으로 학습하게 된다. 

> 실제로 BERT 최종 모델은 NSP task에서 97~98%의 높은 정확도를 보인다.



#### **Pre-training data**

pretraining 단계에서는 **BookCorpus (8억 단어)**, **영어 Wikipedia (25억 단어)** 코퍼스를 사용해 학습한다.





### **Fine-tuning BERT**

BERT를 실제 downstream NLP task에 적용하는 과정은 매우 간단하다. 

1. Pretraining BERT 모델에 task별 output layer만 추가
2. 전체 모델을 해당 task의 supervised learning 데이터로 파인튜닝

Self-Attention 구조 덕분에 입력이 한 문장이든 문장 쌍이든 동일한 형태로 모델에 넣을 수 있으므로, 대부분의 NLP 과제에 별도 구조 변경 없이 투입할 수 있다.

> 시퀀스 내 모든 token이 서로를 참조 → 문장이 2개여도 토큰 순서대로 나열된 하나의 시퀀스 형태이면 상관 No 



그렇다면, 어떻게 하나의 BERT를 여러 종류의 NLP task에 그대로 적용할 수 있을까?

**구조 변경 없이** 입력과 출력 형태만 다르게 가져가면 된다.

#### **Input**

1. **문장 간 유사도·패러프레이징**
   - A와 B, 두 문장이 같은 의미인가?
2. **자연어 추론(NLI; entailment)**
   - 가설(hypothesis)이 전제(premise)로부터 참(entailed)인지?
   - $[\text{CLS}]$ token의 출력벡터 $C$로 두 문장의 관계(예/아니오 등)을 분류
3. **질의응답(QA)**
   - 지문(passage)에서 질문(question)에 대한 답을 찾기
   - 출력 $T_i$들 위에 시작(Start)/끝(End) 위치를 뽑는 별도 벡터를 학습시켜 정답 범위를 예측
4. **단일 문장 분류·시퀀스 태깅**
   - 감성 분석, 개체명 인식 등
   - “문장 A” = 실제 문장, “문장 B” = 빈 시퀀스(∅) — 즉, 문장 하나만 처리
   - 각 token의 출력벡터 $T_i$를 받아 classifier를 통과시켜 label 또는 tag를 예측



이렇게 “A–B 쌍”으로 입력을 통일하면, BERT는 $\text{[CLS] A [SEP] B [SEP]}$ 형태로 **항상 같은 방식**으로 인코딩할 수 있다.



#### **Output**

1. **token-level tasks**
   - 예: 개체명 인식(NER), 시퀀스 태깅, QA에서 정답 범위 예측(start/end) 등
   - 방법: Transformer 마지막 layer에서 얻은 **각 토큰의 벡터**($T₁, T₂, …$)를
     - 시퀀스 태깅 → 각 token 벡터에 tagging용 softmax 분류기 적용
     - QA → token별로 “시작 위치일 확률, 끝 위치일 확률”을 예측하는 벡터 layer 추가
2. **sequence-level tasks**
   - 예: 문장 분류(감성·주제 분류), 자연어 추론(NLI), 문장쌍 판별(NSP) 등
   - 방법: $\text{[CLS]}$ token의 은닉 벡터 $C$만 가져와 **classifier softmax**에 연결

task 특화 아키텍처를 새로 만들 필요 없이, BERT 본체는 동일하게 유지한 채 Output layer 만 바꿔 학습하면 되므로 개발이 수월하다.





## **Experiments**

BERT는 **11가지 NLP task**에 대해 파인튜닝 실험을 수행한다 .

그 중 대표적으로

* 문장 수준의 **GLUE benchmark**(자연어 이해 종합 평가)
* 질의응답 데이터셋 **SQuAD v1.1/v2.0**
* 상식 추론 데이터셋 **SWAG** 

에 대한 성능을 기술한다.



### **GLUE**

**GLUE benchmark**는 문장 혹은 문장쌍을 입력으로 하는 **문장 이해** 중심의 8개 과제 세트이다. 

> The General Language Understanding Evaluation

아래는 $\text{BERT}\_\text{BASE}$와 $\text{BERT}\_\text{LARGE}$ 모델을 fine-tuning하여 결과를 GLUE 리더보드의 기존 최고 성능과 비교한 표이다. 

![스크린샷 2025-07-23 오전 10.43.18](/assets/img/2025-07-24-BERT/t1.png)

$\text{BERT}\_\text{BASE}$ 모델조차도 모든 task에서 당시 최고 성능을 넘었고, $\text{BERT}\_\text{LARGE}$는 평균 정확도에서 기존 최고치 대비 7%p 이상의 큰 향상을 달성했다 . 

특히 이전까지 최고 성능 모델이었던 GPT와 비교하면, $\text{BERT}\_\text{BASE}$는 모델 크기나 구조가 유사함에도 불구하고 GPT를 크게 앞섰다.

> $\text{BERT}_\text{BASE}$ 와 GPT의 차이는 Attention의 방향뿐이므로 성능 차이는 BERT의 양방향 MLM 학습 덕분이라 해석할 수 있다.

또한 $\text{BERT}\_\text{LARGE}$가 소규모 데이터셋 과제들(예: RTE, MRPC 등)에서 $\text{BERT}\_\text{BASE}$ 대비 현저히 높은 점수를 얻었는데 , 이는 사전훈련으로 학습한 풍부한 표현이 부족한 데이터 상황에서도 효과적임을 보여준다. 



### **SQuAD**

**SQuAD**는 지문을 읽고 질문에 대한 답변(text span)을 찾아내는 과제이다. 

> The Stanford Question Answering Dataset

####  **SQuAD v1.1**

10만개의 크라우드 소싱 question-answer pair를 가진 데이터셋이다.



fine-tuning 단계에서, 답의 시작(start) 위치를 예측하기 위해 $S ∈ ℝ^H$ 벡터를, 답의 끝(end) 위치를 예측하기 위해 $ E ∈ ℝ^H$ 벡터를 새로 학습한다.

Transformer의 마지막 레이어에서, 입력 시퀀스 각 토큰 위치 $i$에 대해 **은닉 벡터** $T_i ∈ ℝ^H$를 얻는다.

- 각 토큰 $i$가 **정답 답변의 시작과 끝**일 확률 $P_i$는 다음과 같이 계산된다:

  
  $$
  P_i = \frac{\exp(\square \cdot T_i)}{\displaystyle\sum_{j=1}^{N} \exp(\square \cdot T_j)}, \;\; \square = S \;\text{or}\; E
  $$

- 특정 시작 $i$와 끝 $j$를 묶어 하나의 **답**으로 볼 때, 그 점수는

  
  $$
  \text{score}(i,j) = S \cdot T_i \;+\; E \cdot T_j, \;\; j \ge i
  $$
  
- $\text{score}(i,j)$가 가장 큰 것을 선택

- lr = $5\text{e}-5$, batch size = 32

저자들은  TriviaQA 데이터셋으로 fine-tuning 한 뒤 SQaUD v1.1를 학습시킴으로써 데이터를 보강하여 학습한 모델도 비교했다. 결과는 아래와 같다.

<img src="/assets/img/2025-07-24-BERT/t2.png" alt="스크린샷 2025-07-23 오전 11.39.11" style="zoom:20%;" />

* 공개된 연구 중 대표적인 이전 최고 모델(Published)보다 단일 $\text{BERT}_\text{LARGE}$ 모델(F1 90.9)이 이미 넘어섬

- 앙상블 + 추가 QA 데이터(TriviaQA)를 쓰면, 인간 수준을 넘어서는 EM 87.4 / F1 93.2라는 sota 달성

**pretrained Bidirectional Transformer** 하나만으로도 복잡한 QA task에서 SOTA를 달성할 수 있다는 점을 입증한 결과이다.



#### **SQuAD v2.0**

**SQuAD v2.0**는 SQuAD v1.1에 “정답이 없는 질문”을 추가하여 현실성을 높인 데이터셋이다. 

> 즉, 주어진 지문에 답이 존재하지 않는 경우도 포함되어 있어서, 모델이 no-answer 이라고 판단하는 방법을 학습



1. **답이 없는 질문 처리**

   - SQuAD v1.1에서는 항상 답이 존재

     - 모델은 지문 내 반드시 정답 span(start–end)이 존재

   - SQuAD v2.0에서는 답이 없을 수도 있음

     - 논문에서는 no answer를 $\text{[CLS]}$ 위치를 가리키는 스팬으로 처리

       > 즉, 답이 없으면 시작 위치와 끝 위치 둘 다 $\text{[CLS]}$ index인 0으로 예측하도록 학습

2. **확률 비교로 no-answer 판단**

   - “no-answer” 스팬의 점수는 $\text{score}_\text{null} = S \cdot C + E \cdot C$ 로, 두 벡터($S$와 $E$)를 $\text{[CLS]}$ 의 출력 $C$에 대해 각각 내적한 합으로 정의
   - 모델은 “가장 높은 점수를 내는 span $(j ≥ i)$을 찾은 뒤, 그 스팬 점수와 “no-answer” 점수를 비교하여 **no-answer 여부**를 결정
     * $\underset{\text{token answer span}}{\max_{j \ge i} \bigl(S \cdot T_i + E \cdot T_j\bigr)} < \underset{\text{non-answer score} }{ \text{score}_\text{null} + \tau}$ 일 때, no-answer
     * 여기서 $\tau$는 개발(Dev ; validation이라고 생각해도 무방) 단계에서 F1을 최대로 해 주는 threshold 값

TriviaQA 같은 외부 데이터는 사용하지 않았으며, lr = $5\text{e}-5$, batch size = 48로 fine tuning

<img src="/assets/img/2025-07-24-BERT/t3.png" alt="t3" style="zoom:20%;" />

위 표를 보면, $\text{BERT}_\text{LARGE}$ 단일 모델이 타 모델들에 비해 좋은 성능을 보이고 있음을 알 수 있다.



### **SWAG**

SWAG는 주어진 상황 문장에 이어질만한 문장을 여러 선택지 중 고르는 작업으로,  **상황 기반 상식 추론** 데이터셋이다.

> Situations With Adversarial Generations

주어진 “문장 A”(상황 설명)에 이어질 “문장 B” 후보 4개 중에서 그럴듯한 문장을 고르는 학습이다.



1. $\text{[CLS]}\; \text{A}\; \text{[SEP]} \; \text{B} \; \text{[SEP]}$ 형태의 시퀸스에서 B 후보 4개를 삽입한 Input을 4개 형성
2. Transformer Encoder를 통과시켜 얻은 $\text{[CLS]}$ 위치의 벡터 $C$에, **score 벡터** $W$를 내적하여 score 계산
3. 4가지 후보의 score를 softmax로 정규화하여, 가장 확률이 높은 후보를 정답으로 예측

> task 특화 파라미터는 score 벡터 $W$ 뿐이다.

<img src="/assets/img/2025-07-24-BERT/t4.png" alt="스크린샷 2025-07-23 오후 1.21.45" style="zoom:20%;" />

위 표를 보면, $\text{BERT}_\text{LARGE}$ 단일 모델이 타 모델들에 비해 좋은 성능을 보이고 있음을 알 수 있다.





## **Ablation Studies**

BERT에 사용된 요소들의 중요성을 평가하기 위한 실험이다.



### **Pre-training**

Pre-training 단계에서 주요 설계 요소인 **MLM과 NSP**의 효과를 검증한다  . 

1. **No NSP** : NSP를 제외한 모델

2. **LTR & No NSP** : MLM 대신 GPT처럼 단순 좌->우 LM으로 학습한 모델

3. **LTR & No NSP + BiLSTM** : 2번 모델에 BiLSTM을 추가한 모델

   > LTR 모델의 한계를 보완하기 위해 출력단에 BiLSTM을 추가(양방향성) 

<img src="/assets/img/2025-07-24-BERT/t5.png" alt="t5" style="zoom:33%;" />

*  **No NSP** 

  * 자연어 추론(MNLI, QNLI)와 질의응답(SQuAD) 성능이 **유의미하게 하락**

  → NSP로 문장 간 관계를 미리 학습해둔 것이 문장 관계 과제와 질문응답 성능 상승에 기여

* **LTR & No NSP**

  * 대부분의 과제에서 MLM 모델보다 성능 하락
  * MRPC (문장 유사도)와 SQuAD에서 큰 감소

  → 양방향 문맥 없이 한쪽 방향만 활용했을 때, 특히 **토큰 수준 과제**(QA)와 **문장 pair 과제**에서 큰 성능 저하

* **LTR & No NSP + BiLSTM**

  * SQuAD에서 일부 개선은 되지만, 결국 $\text{BERT}_\text{BASE}$보다 X
  * GLUE task에서는 오히려 하락

  → Pre-training 단계에서부터 양방향 Transformer로 학습된 BERT가 더 효율적



2가지 Pre-training 목표 MLM과 NSP가 BERT 모델 성능 향상에 중요한 역할을 한다는 것을 알 수 있음





### **Others**

* 모델을 키울수록(layer 수, hidden 벡터 size, attention head 수) 일관되게 성능이 개선

* BERT의 사전학습된 **문맥적 임베딩**만 추출해도, downstream NER task에서 **거의 최첨단** 성능

  따라서 

  * 복잡한 구조 변경 없이 “특징만 뽑아 쓰는” **feature-based** 방식

  - 전체 모델을 미세조정하는 **fine-tuning** 방식

  이 둘 모두 BERT를 효과적으로 활용하는 합리적인 옵션임이 입증

  



## **Conclusion**

BERT 논문은 **언어 모델 사전훈련**을 통한 전이학습의 가능성을 한 단계 끌어올린 획기적인 연구로 평가된다. 

* **Bidirection pretrain**이 얼마나 큰 효과를 갖는지 처음으로 입증 
* 하나의 통합 모델이 **문장 수준과 토큰 수준**의 광범위한 NLP 과제를 모두 sota 성능으로 해결



BERT의 설계들 – MLM을 통한 양방향 학습, NSP를 통한 문장 관계 학습, Transformer 인코더 아키텍처의 활용 – 은 각각 해당 문제(단방향성 제약, 문장 연결성 부족 등)를 정확히 짚어내고 해결한 것으로 드러났다. 

특히 GPT 등의 선행 모델과 비교하여, BERT는 **bidirectional LM**이라는 개념적 단순 변화만으로도 **압도적인 성능 향상**을 이루어낸 점에서 의의가 있으며, 복잡한 task별 구조 없이 fine-tuning만으로 최고 성능을 얻어냈다는 것은, 사전훈련된 언어 표현의 **범용성**과 **표현력**이 그만큼 뛰어나다는 증거이다. 

BERT의 공개 이후 NLP 분야에는 유사한 **Pretrain-Finetuning** 패러다임이 폭발적으로 확산되었고 수많은 후속 연구가 등장했다.

정리하면, *“BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”* 논문은 **양방향 트랜스포머 사전훈련**의 개념을 제시하고 성공을 입증함으로써, 현대 NLP 모델의 **새로운 표준을 확립**한 기념비적인 작업이라 할 수 있다.







## 부록 : 11가지 NLP tasks



### **A. GLUE 벤치마크 내 8개 과제 (Appendix B.1)**

1. **MNLI (Multi-Genre Natural Language Inference)**
   - 서로 다른 장르의 문장 쌍에 대해 “두 번째 문장이 첫 번째 문장으로부터 참·모순·중립인지” 분류
   - 대규모(약 392K 샘플)로, 언어 추론 능력 평가
2. **QQP (Quora Question Pairs)**
   - Quora에 올라온 질문 쌍이 의미상 같은 질문인지(중복 질문 여부) 이진 분류
   - 약 363K 쌍
3. **QNLI (Question Natural Language Inference)**
   - SQuAD 질문-문장 쌍을 이진 분류 형태로 변환한 과제
   - “문장(passage) 안에 질문(question)의 답이 포함되어 있는가?” 판단, 약 108K 샘플
4. **SST-2 (Stanford Sentiment Treebank-2)**
   - 영화 리뷰 문장의 긍정/부정 감성 이진 분류
   - 문장 단위 감성 분석, 약 67K 샘플
5. **CoLA (Corpus of Linguistic Acceptability)**
   - 문장이 문법적으로 허용(acceptable)되는지 여부를 판별하는 이진 분류
   - 비교적 작은 규모(약 8.5K 샘플)
6. **STS-B (Semantic Textual Similarity Benchmark)**
   - 두 문장의 의미 유사도를 0–5 점수로 평가(회귀)
   - 약 5.7K 쌍, Spearman 상관계수로 성능 측정
7. **MRPC (Microsoft Research Paraphrase Corpus)**
   - 문장 쌍이 의역(paraphrase)인지 여부 이진 분류
   - 약 3.6K 샘플
8. **RTE (Recognizing Textual Entailment)**
   - PASCAL RTE 챌린지 데이터로, 두 문장의 함축(entailment) 여부 이진 분류
   - 약 2.5K 샘플

### **B. QA & 추론 과제 3개**

1. **SQuAD v1.1 (Stanford Question Answering Dataset 1.1)**

   - 질문에 해당하는 짧은 정답 토큰(span)을 지문(context)에서 찾는 추출형 QA
   - 약 100K 질문-답변 쌍

2. **SQuAD v2.0**

   - v1.1에 “답이 지문에 없는(no-answer)” 예시를 추가한 버전
   - 모델이 답의 시작-끝을 예측하거나 “답 없음”을 판단

3. **SWAG (Situations With Adversarial Generations)**

   - 상황(situation) 문장 A에 이어질 “가장 그럴듯한” 문장 B를 4지선다로 고르는 상식 추론
   - 약 113K 예시, grounded common-sense inference 평가

   

이 11개 과제는 **문장 단위 분류**부터 **문장 관계 판단**, **문맥 기반 질의응답**, **상식 추론**까지 매우 다양한 NLP 능력을 포괄합니다. BERT는 이 모두에서 **아키텍처 변경 없이** 동일한 모델을 **fine-tuning**만으로 적용하여 일관되게 최첨단 성능을 달성했습니다.
