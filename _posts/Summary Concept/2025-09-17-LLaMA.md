---
title: "LLaMA, LLaVA, ..."
date: 2025-09-17 16:02:43 +/-0000
categories: [Study]
tags: [LLM]   
use_math: true  # TAG names should always be lowercase
---

# **LLaMA와 다른 대표 언어모델들의 차이점**



- **사전학습 목표**: BERT는 Masked Language Modeling(입력 단어의 15%를 마스킹한 후 예측)과 Next Sentence Prediction(NSP)를 사용하여 양방향 문맥 이해를 학습한다 . 반면 GPT·LLaMA 계열은 *오토리그레시브* 방식으로, 주어진 앞선 단어들로부터 다음 단어(토큰)를 예측하는 방식으로만 학습된다  .
- **아키텍처**: BERT는 *인코더*만으로 구성된 다층 Transformer로, 문장 전체를 동시에 양방향으로 인식할 수 있다 . 이에 반해 GPT와 LLaMA는 *디코더*만 사용하는 Autoregressive 구조로, 텍스트를 왼쪽부터 오른쪽 순차적으로 생성한다  . LLaMA는 GPT와 유사한 디코더 구조를 기본으로 하되, Rotary Positional Embedding(RoPE) 도입 등 효율화 기법으로 적은 파라미터에도 우수한 성능을 낸다 .
- **용도 및 강점**: BERT는 전체 문맥을 깊이 이해하기 때문에 문장 분류, 질의응답, 개체명 인식 등 *언어 이해* 작업에 강점이 있다 . GPT·LLaMA는 우수한 문장 생성 능력을 가지며 대화·텍스트 생성 등 *언어 생성* 응용에 적합하다  . 예를 들어, GPT-4는 멀티모달 지원도 하지만 본질적으로는 문장 생성에 특화되어 있다. LLaMA도 텍스트 생성 성능이 높으며, 특히 Meta는 작은 모델로도 경쟁력 있는 성능을 제공해 연구·개발 커뮤니티의 접근성을 높인 점이 특징이다  .
- **모델 유형**: 한마디로 정리하면, **BERT는 인코더 전용**, **GPT와 LLaMA는 디코더 전용** 구조이다. BERT는 인코더 특성상 텍스트 생성엔 부적합하며, GPT/LLaMA는 디코더 특성상 주로 자동생성에 사용된다  . 이처럼 구조와 학습 방식의 차이로 각 모델은 다른 용도에 최적화되어 있다.







## **LLaVA와 LLaMA의 차이점**





- **모델 구조**: LLaMA는 텍스트 전용 대형 언어 모델(단일 모달)으로, 이미지나 비디오를 직접 처리하지 않는다. 반면 **LLaVA**(Large Language and Vision Assistant)는 LLaMA 기반의 *멀티모달* 모델로, **비전 인코더**를 결합한 구조를 가진다. 구체적으로, LLaVA는 CLIP의 비전 인코더(ViT 등)와 LLaMA 계열 언어 모델(Vicuna)을 연결한다 .
- **입력 처리**: LLaMA는 텍스트만 입력받지만, LLaVA는 이미지를 포함한 시각-언어 지시를 입력으로 받는다. LLaVA는 CLIP로부터 추출된 이미지 특징(feature)을 선형 투영하여 LLaMA(Vicuna)의 입력(soft prompt)으로 사용함으로써, 이미지를 텍스트처럼 언어 모델에 이해시킨다. 즉, 비전-언어 융합 레이어를 통해 **이미지 특징을 언어 공간**에 매핑한다 .
- **학습 데이터 및 방법**: LLaMA는 텍스트 대규모 말뭉치만으로 사전학습되었다. LLaVA는 **시각-언어 지시 학습**(visual instruction tuning) 방식으로, GPT-4로 생성된 이미지+텍스트 지시사항 데이터로 *엔드투엔드 파인튜닝*된다  . 예를 들어, LLaVA 논문에서는 GPT-4를 이용해 이미지 설명·질문 등의 형식으로 된 멀티모달 지시 데이터를 만들고, 이를 통해 LLaMA(Vicuna)를 CLIP 인코더와 함께 미세조정한다 .
- **멀티모달 설계**: 요약하면, **LLaVA는 LLaMA를 확장해 시각 정보를 처리하도록 설계된 모델**이다. 즉, LLaMA의 디코더 구조 위에 CLIP 비전 인코더를 연결하고, 이미지와 텍스트를 동시에 입력받아 종합적으로 처리할 수 있다  . LLaMA는 순수 언어 모델인 반면, LLaVA는 이 구조를 통해 시각-언어 통합 이해와 생성이 가능하다.







## **LLaMA 기반 멀티모달 확장 연구 비교**





LLaMA를 기반으로 한 대표적 멀티모달 모델들은 비전 인코더, 연결 방식, 학습법에서 차이가 있다. 주요 모델을 구조적 관점에서 비교하면 다음과 같다:

| **모델**          | **비전 인코더**                | **언어 모델(기반)** | **연결 방식 및 특징**                                        | **학습 방식/데이터**                                         |
| ----------------- | ------------------------------ | ------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **LLaVA**         | CLIP 비전 인코더(ViT)          | Vicuna (LLaMA 계열) | CLIP 출력 특징을 선형 투영하여 LLaMA 입력(soft prompt)으로 사용 | GPT-4 생성 시각-언어 지시 데이터로 end-to-end 파인튜닝       |
| **MiniGPT-4**     | BLIP-2 인코더 (ViT + Q-Former) | Vicuna (LLaMA 계열) | Q-Former 출력에 1개 레이어 선형 투영기를 통해 LLM의 soft prompt로 연결 | 1) Projection 학습: 방대한 이미지-텍스트 정렬, 2) 고품질 이미지 설명 데이터로 미세조정 |
| **LLaMA-Adapter** | (예: CLIP ViT 기반)            | LLaMA (7B 등)       | LLaMA의 self-attention 층에 zero-init 어댑터(attention) 추가 , 이미지 특징을 어댑터를 통해 주입 | 이미지 포함 Self-Instruct 데이터(예: 52K 지시문)로 미세조정  |
| **LLaMA-VID**     | EVA-G (ViT) + Q-Former         | Vicuna (LLaMA 계열) | 각 비디오 프레임을 ‘컨텍스트 토큰’ + ‘콘텐츠 토큰’ 두 개로 압축하여 LLM 입력 , (cross-attention 방식) | 멀티모달 및 비디오 지시 데이터로 단계별 훈련, 긴 비디오용 토큰 압축 기법 적용 |



- **LLaVA**는 CLIP(ViT)와 Vicuna(LLaMA)를 연결하며, CLIP의 출력 특징을 LLaMA의 입력으로 투영하는 구조다. GPT-4 기반으로 만든 이미지+텍스트 지시 데이터로 End-to-end 학습한다 .
- **MiniGPT-4**는 BLIP-2 인코더(ViT+Q-Former)를 사용하며, 출력된 이미지 특징을 1-layer 선형 프로젝터로 Vicuna에 제공한다 . 학습은 두 단계로 구성되어, 먼저 Projection layer를 이미지-텍스트 정렬 데이터로 학습한 후, 고품질 이미지 설명 데이터로 모델을 미세 조정한다 .
- **LLaMA-Adapter**는 LLaMA 내부에 소규모 어댑터(1.2M 파라미터)를 삽입하는 방식으로 확장한다 . 제로 초기화된 어텐션 게이트를 통해 이미지 인코더의 정보를 LLaMA에 주입하며, 이미지 포함 지시문 데이터로 미세조정한다  .
- **LLaMA-VID**는 EVA-G 비전인코더와 Q-Former를 사용하여, 각 프레임을 **컨텍스트 토큰**(지시문 관련 시각 정보 요약)과 **콘텐츠 토큰**(주요 시각 정보) 두 토큰으로 변환한다 . 이렇게 변환된 토큰을 Vicuna(LLaMA)로 입력해 긴 비디오도 처리하며, 멀티모달 지시 및 비디오 데이터를 단계적으로 학습한다  .





이들 모델은 모두 **LLaMA 기반 언어 모델** 위에 비전 모듈을 추가한다는 점에서 공통되지만, 비전 인코더 종류와 연결 방식, 학습 데이터에 차이가 있다. 예를 들어 MiniGPT-4는 BLIP-2 인코더를, LLaVA/LLaMA-Adapter는 CLIP 기반 인코더를, LLaMA-VID는 EVA-G/Vit 기반 인코더를 사용한다. 연결 방식도 LLaVA·MiniGPT-4는 특징 벡터를 투영해 soft prompt로 입력하는 반면, LLaMA-Adapter는 어텐션 어댑터를, LLaMA-VID는 토큰 생성 방식을 이용한다    . 각 모델은 이러한 구조를 바탕으로 공개된 멀티모달 데이터나 인공적으로 생성한 데이터로 추가 학습함으로써 시각-언어 이해 능력을 확장한다.

